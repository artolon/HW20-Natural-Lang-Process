{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60175f8",
   "metadata": {},
   "source": [
    "# Week 20: Natural Language Processing\n",
    "### February 10, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6b642",
   "metadata": {},
   "source": [
    "**1.\tWrite a python program (not a Jupyter notebook, but a py file you run from the command line) that accepts the cats_txt.txt file as input and counts the frequency of all words and punctuation in that text file, ordered by frequency. Make sure to handle capital and lowercase versions of words and count them together.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c861b",
   "metadata": {},
   "source": [
    "**2.\tDocument how to run the program you created in question 1 in a readme.md file in your repo. Be as clear as possible. Use proper markdown, and consider using screenshots. Be sure to briefly discuss why this kind of exercise might be helpful for NLP in your markdown.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592be78",
   "metadata": {},
   "source": [
    "### In a Jupyter notebook: \n",
    "\n",
    "**3.\tLoad the tokenized Paradise Lost from the Gutenberg Corpus in NLTK.* https://www.nltk.org/book/ch02.html . Stem or lemmatize the words and find counts. Select the top 20 words and create a histogram. Exclude stop words and make sure you are including words of all capitalizations in your count. If there are any meaningless “words” (“thus” and single letters, etc.) that are produced in your list or top words, alter your logic to exclude them. Specify why you chose stemming or lemmatization.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd59414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Paradise', 'Lost', 'by', 'John', 'Milton', ...]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Get words from Paradise lost\n",
    "lost_words = nltk.corpus.gutenberg.words('milton-paradise.txt')\n",
    "# View the first few words\n",
    "print(lost_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91017dd",
   "metadata": {},
   "source": [
    "Before diving in, I wanted to apply *both* lemmatization and stemming to a subset of words. I wanted to print those results and compare them to the original. This helps provide a better sense of their differences.\n",
    "\n",
    "I experimented with a few different subsets of text. I decided to use the stemmer because (as the same suggests) this was shortening more words to their base (for example, \"heavenly\" to \"heaven\"). Contrastingly, the lemmatizer was keeping the full word, including capitalization. When it comes to the analysis of top 20 words, I am personally not as interested in the nuance that lemmatizing provides. I would rather see the base meaning of the word (i.e. I was words like \"heavenly\" and \"heaven\" to count the same). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0af6d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the word lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Instantiate the word stemmer\n",
    "stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf005d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'regain', 'the', 'blissful', 'seat', ',', 'Sing', ',', 'Heavenly', 'Muse', ',', 'that', ',', 'on', 'the', 'secret', 'top', 'Of', 'Oreb', ',', 'or', 'of', 'Sinai', ',', 'didst', 'inspire', 'That', 'shepherd', 'who', 'first', 'taught', 'the', 'chosen', 'seed', 'In', 'the', 'beginning', 'how', 'the', 'heavens', 'and', 'earth', 'Rose', 'out', 'of', 'Chaos', ':', 'or', ',', 'if']\n",
      "['and', 'regain', 'the', 'blissful', 'seat', ',', 'Sing', ',', 'Heavenly', 'Muse', ',', 'that', ',', 'on', 'the', 'secret', 'top', 'Of', 'Oreb', ',', 'or', 'of', 'Sinai', ',', 'didst', 'inspire', 'That', 'shepherd', 'who', 'first', 'taught', 'the', 'chosen', 'seed', 'In', 'the', 'beginning', 'how', 'the', 'heaven', 'and', 'earth', 'Rose', 'out', 'of', 'Chaos', ':', 'or', ',', 'if']\n",
      "['and', 'regain', 'the', 'bliss', 'seat', ',', 'sing', ',', 'heaven', 'muse', ',', 'that', ',', 'on', 'the', 'secret', 'top', 'of', 'oreb', ',', 'or', 'of', 'sinai', ',', 'didst', 'inspir', 'that', 'shepherd', 'who', 'first', 'taught', 'the', 'chosen', 'seed', 'in', 'the', 'begin', 'how', 'the', 'heaven', 'and', 'earth', 'rose', 'out', 'of', 'chao', ':', 'or', ',', 'if']\n"
     ]
    }
   ],
   "source": [
    "# Create a subset of words to run some code and see what it looks like\n",
    "sub = lost_words[50:100]\n",
    "print(sub)\n",
    "# See how lemmatizer works\n",
    "sub_lem = [lemmatizer.lemmatize(word) for word in sub]\n",
    "print(sub_lem)\n",
    "# See how stemmer works\n",
    "sub_stem = [stemmer.stem(word) for word in sub]\n",
    "print(sub_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d212bd",
   "metadata": {},
   "source": [
    "Now, I will apply stemming to all of the words in Paradise Lost, rather than just the subset. In order to do that, I will first need to lowercase all of the words, ensure everything is alpha (not numeric), and then remove the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5b36221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of additional stop words that I want to remove from analysis\n",
    "old_english = ['thou', 'thy', 'thee', 'thus', 'shall', 'yet', 'though', 'may', 'till']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "948126b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('heaven', 485), ('god', 316), ('earth', 228), ('us', 187), ('man', 178), ('first', 175), ('high', 159), ('day', 157), ('one', 142), ('power', 133), ('like', 132), ('son', 132), ('great', 130), ('far', 129), ('death', 127), ('world', 122), ('light', 122), ('good', 122), ('hell', 119), ('night', 117)]\n"
     ]
    }
   ],
   "source": [
    "# Ensure everything is lower case\n",
    "lost_low = [word.lower() for word in lost_words]\n",
    "\n",
    "# Ensure everything is alpha only\n",
    "lost_alpha = [word for word in lost_low if word.isalpha()]\n",
    "\n",
    "# Define the stop words\n",
    "english_stops = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Remove the stop words\n",
    "lost_stop = [word for word in lost_alpha if word not in english_stops and word not in old_english]\n",
    "\n",
    "# Iterate through the entire text and stem the words in a new list called \"lost_stem\"\n",
    "lost_stem = [stemmer.stem(word) for word in lost_stop]\n",
    "\n",
    "# Count all of the words\n",
    "lost_count = Counter(lost_stem)\n",
    "\n",
    "# Print the 20 most common words\n",
    "lost20 = lost_count.most_common(20)\n",
    "print(lost20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66ff7b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unpack the tuples\n",
    "word, count = zip(*lost20)\n",
    "\n",
    "# Set up figure size\n",
    "fig = plt.figure(figsize = (17,7))\n",
    "# Plot word and count\n",
    "plt.bar(word, count)\n",
    "# Set up the labels\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Most Common Words in Paradise Lost')\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae8c10",
   "metadata": {},
   "source": [
    "**4.\tPerform Vader Sentiment Analysis on the book. Find the 5 most negative, 5 most positive, and 5 most neutral sentences in Paradise Lost*. http://www.nltk.org/howto/sentiment.html . This may take a while to run, so you can always start with a small subset of the data (100 sentences) and then once your code works as expected, expand it to the whole book and let it run.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7308dc1f",
   "metadata": {},
   "source": [
    "First, I need to extract all of the sentences from Paradise Lost. The output will be a list of lists. Each sub-list will contain words/\"tokens\" which comprise one sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "023a0f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'Paradise', 'Lost', 'by', 'John', 'Milton', '1667', ']'], ['Book', 'I'], ['Of', 'Man', \"'\", 's', 'first', 'disobedience', ',', 'and', 'the', 'fruit', 'Of', 'that', 'forbidden', 'tree', 'whose', 'mortal', 'taste', 'Brought', 'death', 'into', 'the', 'World', ',', 'and', 'all', 'our', 'woe', ',', 'With', 'loss', 'of', 'Eden', ',', 'till', 'one', 'greater', 'Man', 'Restore', 'us', ',', 'and', 'regain', 'the', 'blissful', 'seat', ',', 'Sing', ',', 'Heavenly', 'Muse', ',', 'that', ',', 'on', 'the', 'secret', 'top', 'Of', 'Oreb', ',', 'or', 'of', 'Sinai', ',', 'didst', 'inspire', 'That', 'shepherd', 'who', 'first', 'taught', 'the', 'chosen', 'seed', 'In', 'the', 'beginning', 'how', 'the', 'heavens', 'and', 'earth', 'Rose', 'out', 'of', 'Chaos', ':', 'or', ',', 'if', 'Sion', 'hill', 'Delight', 'thee', 'more', ',', 'and', 'Siloa', \"'\", 's', 'brook', 'that', 'flowed', 'Fast', 'by', 'the', 'oracle', 'of', 'God', ',', 'I', 'thence', 'Invoke', 'thy', 'aid', 'to', 'my', 'adventurous', 'song', ',', 'That', 'with', 'no', 'middle', 'flight', 'intends', 'to', 'soar', 'Above', 'th', \"'\", 'Aonian', 'mount', ',', 'while', 'it', 'pursues', 'Things', 'unattempted', 'yet', 'in', 'prose', 'or', 'rhyme', '.'], ['And', 'chiefly', 'thou', ',', 'O', 'Spirit', ',', 'that', 'dost', 'prefer', 'Before', 'all', 'temples', 'th', \"'\", 'upright', 'heart', 'and', 'pure', ',', 'Instruct', 'me', ',', 'for', 'thou', 'know', \"'\", 'st', ';', 'thou', 'from', 'the', 'first', 'Wast', 'present', ',', 'and', ',', 'with', 'mighty', 'wings', 'outspread', ',', 'Dove', '-', 'like', 'sat', \"'\", 'st', 'brooding', 'on', 'the', 'vast', 'Abyss', ',', 'And', 'mad', \"'\", 'st', 'it', 'pregnant', ':', 'what', 'in', 'me', 'is', 'dark', 'Illumine', ',', 'what', 'is', 'low', 'raise', 'and', 'support', ';', 'That', ',', 'to', 'the', 'height', 'of', 'this', 'great', 'argument', ',', 'I', 'may', 'assert', 'Eternal', 'Providence', ',', 'And', 'justify', 'the', 'ways', 'of', 'God', 'to', 'men', '.'], ['Say', 'first', '--', 'for', 'Heaven', 'hides', 'nothing', 'from', 'thy', 'view', ',', 'Nor', 'the', 'deep', 'tract', 'of', 'Hell', '--', 'say', 'first', 'what', 'cause', 'Moved', 'our', 'grand', 'parents', ',', 'in', 'that', 'happy', 'state', ',', 'Favoured', 'of', 'Heaven', 'so', 'highly', ',', 'to', 'fall', 'off', 'From', 'their', 'Creator', ',', 'and', 'transgress', 'his', 'will', 'For', 'one', 'restraint', ',', 'lords', 'of', 'the', 'World', 'besides', '.'], ['Who', 'first', 'seduced', 'them', 'to', 'that', 'foul', 'revolt', '?'], ['Th', \"'\", 'infernal', 'Serpent', ';', 'he', 'it', 'was', 'whose', 'guile', ',', 'Stirred', 'up', 'with', 'envy', 'and', 'revenge', ',', 'deceived', 'The', 'mother', 'of', 'mankind', ',', 'what', 'time', 'his', 'pride', 'Had', 'cast', 'him', 'out', 'from', 'Heaven', ',', 'with', 'all', 'his', 'host', 'Of', 'rebel', 'Angels', ',', 'by', 'whose', 'aid', ',', 'aspiring', 'To', 'set', 'himself', 'in', 'glory', 'above', 'his', 'peers', ',', 'He', 'trusted', 'to', 'have', 'equalled', 'the', 'Most', 'High', ',', 'If', 'he', 'opposed', ',', 'and', 'with', 'ambitious', 'aim', 'Against', 'the', 'throne', 'and', 'monarchy', 'of', 'God', ',', 'Raised', 'impious', 'war', 'in', 'Heaven', 'and', 'battle', 'proud', ',', 'With', 'vain', 'attempt', '.'], ['Him', 'the', 'Almighty', 'Power', 'Hurled', 'headlong', 'flaming', 'from', 'th', \"'\", 'ethereal', 'sky', ',', 'With', 'hideous', 'ruin', 'and', 'combustion', ',', 'down', 'To', 'bottomless', 'perdition', ',', 'there', 'to', 'dwell', 'In', 'adamantine', 'chains', 'and', 'penal', 'fire', ',', 'Who', 'durst', 'defy', 'th', \"'\", 'Omnipotent', 'to', 'arms', '.'], ['Nine', 'times', 'the', 'space', 'that', 'measures', 'day', 'and', 'night', 'To', 'mortal', 'men', ',', 'he', ',', 'with', 'his', 'horrid', 'crew', ',', 'Lay', 'vanquished', ',', 'rolling', 'in', 'the', 'fiery', 'gulf', ',', 'Confounded', ',', 'though', 'immortal', '.'], ['But', 'his', 'doom', 'Reserved', 'him', 'to', 'more', 'wrath', ';', 'for', 'now', 'the', 'thought', 'Both', 'of', 'lost', 'happiness', 'and', 'lasting', 'pain', 'Torments', 'him', ':', 'round', 'he', 'throws', 'his', 'baleful', 'eyes', ',', 'That', 'witnessed', 'huge', 'affliction', 'and', 'dismay', ',', 'Mixed', 'with', 'obdurate', 'pride', 'and', 'steadfast', 'hate', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Get sentences from Paradise lost\n",
    "lost_sents = nltk.corpus.gutenberg.sents('milton-paradise.txt')\n",
    "# Print only the first few sentences as a test\n",
    "print(lost_sents[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e849dbff",
   "metadata": {},
   "source": [
    "In order to properly analyze the sentences, we will need to paste the tokens back together. To do this, I need to iterate through the lists, join words together, and then remove the unwanted spaces. I will then append this to a *new* list. The *new* list will still be a giant list of lists. But each list will contain full *sentences,* rather than individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b9fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the words back into actual sentences\n",
    "\n",
    "# Empty list to append to\n",
    "sentence_list = []    \n",
    "\n",
    "# Iterate through the lists within sentences\n",
    "for i, word_list in enumerate(lost_sents):\n",
    "    # Concatenate the words together and remove spaces from punctuation\n",
    "    sentence = ' '.join(word_list).replace(' , ', ', ').replace(' : ', ': ').replace(\" ' \", \"'\").replace(\" ,'\", \",'\").replace(' ?', \"?\").replace(' .', '.').replace(' ;', ';').replace(\" !\", '!').replace('( ', '(').replace(' )', ')')\n",
    "    # append them to our empty list as a list (list of lists)\n",
    "    sentence_list.append([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024fd8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Of Man's first disobedience, and the fruit Of that forbidden tree whose mortal taste Brought death into the World, and all our woe, With loss of Eden, till one greater Man Restore us, and regain the blissful seat, Sing, Heavenly Muse, that, on the secret top Of Oreb, or of Sinai, didst inspire That shepherd who first taught the chosen seed In the beginning how the heavens and earth Rose out of Chaos: or, if Sion hill Delight thee more, and Siloa's brook that flowed Fast by the oracle of God, I thence Invoke thy aid to my adventurous song, That with no middle flight intends to soar Above th'Aonian mount, while it pursues Things unattempted yet in prose or rhyme.\"]\n"
     ]
    }
   ],
   "source": [
    "# View a sample sentence\n",
    "print(sentence_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ff629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For code-testing purposes, extract a slice of sentences to view\n",
    "sample = sentence_list[:101]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c290e42",
   "metadata": {},
   "source": [
    "Now that I have the complete sentences, I can run the sentiment analysis! I will have to iterate through all of the sentences. While doing so, I will calculate the polarity score and append it to yet another list! The \"sentiments\" list will contain all of the polarity scores from each individual sentence. The output of \"sentiments\" will be one giant list of lists. And each nested list will contain a *dictionary* with the polarity results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea16a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the sentiment analyzer\n",
    "sent = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Where we will append our sentiments\n",
    "    # This will end up being a list of lists that each contain a dictionary...\n",
    "sentiment_list = []\n",
    "\n",
    "# iterate through the sample:\n",
    "# real deal now! Change sample to sentence_list\n",
    "for i, s in enumerate(sentence_list):\n",
    "    # list comprehension to get polarity scores for every sentence in the sample\n",
    "    polarity = [sent.polarity_scores(s) for s in sentence_list[i]]\n",
    "    # append the scores to the empty sentiment list\n",
    "    sentiment_list.append(polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba1f16",
   "metadata": {},
   "source": [
    "To create a data frame, I need to transform my list of lists (with dictionaries) into *just* a list of dictionaries. Code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12d83069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.219</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.8885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.054</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.8546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.8689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound\n",
       "0  0.219  0.381  0.400    0.4404\n",
       "1  0.000  1.000  0.000    0.0000\n",
       "2  0.117  0.689  0.194    0.8885\n",
       "3  0.054  0.771  0.175    0.8546\n",
       "4  0.093  0.668  0.239    0.8689"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to make this ONE list of dictionaries, rather than nested lists...\n",
    "\n",
    "# append dictionaries here\n",
    "dct_list = []\n",
    "\n",
    "# Iterate through the list of lists of dictionaries\n",
    "for score in sentiment_list:\n",
    "    # extract only the dictionaries from each list\n",
    "    for dct in score:\n",
    "        # append the dictionaries to new list\n",
    "        dct_list.append(dct)\n",
    "\n",
    "# Now, we can convert the dictionaries to a data frame\n",
    "polarity_df = pd.DataFrame(dct_list)\n",
    "# View sample data frame\n",
    "polarity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87616f6e",
   "metadata": {},
   "source": [
    "Finally, I will sort the polarity data frame by positive values (descending), so that we find the most positive results. I will limit the results to only 5. I will then extract the index and transform that into a list so that we can later use it for iterating. I'll repeat this for the negative and neutral columns as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abf15d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[605, 1441, 908, 828, 1233]\n",
      "      neg    neu    pos  compound\n",
      "605   0.0  0.000  1.000    0.5255\n",
      "1441  0.0  0.000  1.000    0.5562\n",
      "908   0.0  0.000  1.000    0.5255\n",
      "828   0.0  0.000  1.000    0.5562\n",
      "1233  0.0  0.162  0.838    0.9381\n"
     ]
    }
   ],
   "source": [
    "# Find the top 5 most positive lists\n",
    "pos_list = polarity_df.sort_values('pos', ascending=False).head(5).index.tolist()\n",
    "print(pos_list)\n",
    "\n",
    "print(polarity_df.sort_values('pos', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "599e1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[153, 512, 1689, 478, 175]\n",
      "        neg    neu  pos  compound\n",
      "153   1.000  0.000  0.0   -0.3595\n",
      "512   1.000  0.000  0.0   -0.7088\n",
      "1689  1.000  0.000  0.0   -0.3382\n",
      "478   0.777  0.223  0.0   -0.5411\n",
      "175   0.772  0.228  0.0   -0.5255\n"
     ]
    }
   ],
   "source": [
    "# Find the top 5 most negatives lists\n",
    "neg_list = polarity_df.sort_values('neg', ascending=False).head(5).index.tolist()\n",
    "print(neg_list)\n",
    "\n",
    "print(polarity_df.sort_values('neg', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6acc531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1850, 515, 555, 550, 1398]\n",
      "      neg  neu  pos  compound\n",
      "1850  0.0  1.0  0.0       0.0\n",
      "515   0.0  1.0  0.0       0.0\n",
      "555   0.0  1.0  0.0       0.0\n",
      "550   0.0  1.0  0.0       0.0\n",
      "1398  0.0  1.0  0.0       0.0\n"
     ]
    }
   ],
   "source": [
    "# Find the top 5 most neutral lists\n",
    "neu_list = polarity_df.sort_values('neu', ascending=False).head(5).index.tolist()\n",
    "print(neu_list)\n",
    "\n",
    "print(polarity_df.sort_values('neu', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1fdbb2",
   "metadata": {},
   "source": [
    "Now, use the above index values to extract the corresponding sentences from Paradise Lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd985a5a",
   "metadata": {},
   "source": [
    "**Most Positive Sentences in Paradise Lost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "929fba35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O friends!']\n",
      "['O Heaven!']\n",
      "['O Friends!']\n",
      "['O Heaven!']\n",
      "['She fair, divinely fair, fit love for Gods!']\n"
     ]
    }
   ],
   "source": [
    "# Positive sentences\n",
    "for i in pos_list:\n",
    "    print(sentence_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b4b15",
   "metadata": {},
   "source": [
    "**Most Negative Sentences in Paradise Lost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4374112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No!']\n",
      "['O Hell!']\n",
      "['Alas!']\n",
      "['Me miserable!']\n",
      "['Sad cure!']\n"
     ]
    }
   ],
   "source": [
    "# Negative sentences\n",
    "for i in neg_list:\n",
    "    print(sentence_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df388d32",
   "metadata": {},
   "source": [
    "**Most Neutral Sentences in Paradise Lost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cf77d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ The End ]\\x1a\\x1a']\n",
      "['Ah!']\n",
      "['When Adam thus to Eve.']\n",
      "['To whom the winged warriour thus returned.']\n",
      "['Hadst thou been firm and fixed in thy dissent, Neither had I transgressed, nor thou with me.']\n"
     ]
    }
   ],
   "source": [
    "# Neutral sentences\n",
    "for i in neu_list:\n",
    "    print(sentence_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c08ff9",
   "metadata": {},
   "source": [
    "**5.\tExplain your findings from the previous question. Are the sentences and their sentiment analysis scores correct? Explain why or why not.** \n",
    "\n",
    "*you can pick another book in the Gutenberg Corpus if you’re feeling adventurous or curious.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ea905",
   "metadata": {},
   "source": [
    "*Positive:*\n",
    "\n",
    "The first 4 sentences were 100% positive and the last was 84%. I am not at all surprised by the results of the first 4 sentences. This is because they were each extremely short, consisting of only two words (e.g. 'O' friends!'). Moreover, the specific words (friends and heaven) both have very positive connotations in the English language. So on their own, it makes sense that the whole sentence would be 100% positive. The last sentence, \"She fair, divinely fair, fit love for Gods!\" is also quite positive. \"fair,\" \"love,\" \"divine,\" and \"Gods,\" all have positive connotations in the English language, and the other words in the sentence are fairly neutral. Overall, unsurprising results. \n",
    "\n",
    "*Negative:*\n",
    "\n",
    "The first 3 sentences were 100% negative and the last two were 78% and 77%, respectively. Similarly to the positive results, the negative results comprised of extremely brief sentences. The first two, \"No!\" and \"O' Hell!\" are both firmly negative statements, so the 100% negative is unsurprising. Likewise, \"Alas!\" on its own would have a negative sentiment as well. I am surprised that the last statement \"sad cure!\" was in the top 5. \"Sad\" is clearly a negative word, but \"cure\" is quite positive. Considering there are only two words, I wouldn't have thought that \"sad\" would have had as strong of a weight. So that was interesting.  \n",
    "\n",
    "*Neutral:*\n",
    "\n",
    "All 4 sentences were 100% neutral. This result was definitely the most surprising to me! The first 3 sentences are fairly neutral, so that part makes sense. The 4th sentence contains \"winged\" (which could arguably be positive) and \"warriour\" (which could be negative or positive, depending on the context). As a whole, the sentence *does* feel fairly neutral, but I am just surprised it is 100%. The 4th sentence definitely does not seem neutral to me, so I am very surprised by that result. I wonder if the sentiment analysis does not do as well with old English, and perhaps that hinders the results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
